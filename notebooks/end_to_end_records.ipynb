{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a1",
   "metadata": {},
   "source": [
    "# Traverse: End-to-End (Records CSV)\n",
    "\n",
    "Build a genre/style co-occurrence graph directly from a records CSV.\n",
    "The timeline is based on **release year** rather than listening history.\n",
    "\n",
    "**Prerequisites:**\n",
    "```bash\n",
    "pip install -e \".[dev]\"\n",
    "cd src/traverse/cosmograph/app && npm install && npm run build\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2",
   "metadata": {},
   "source": [
    "## 1. Configuration\n",
    "\n",
    "Update these to match your local setup."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "RECORDS_CSV = Path(r\"C:\\Users\\xtrem\\Documents\\Datasets\\records.csv\")\n",
    "YEAR_MIN = 1860   # clamp: drop years before this\n",
    "YEAR_MAX = 2025   # clamp: drop years after this\n",
    "CHUNKSIZE = 200_000\n",
    "MIN_COOCCURRENCE = 2\n",
    "MAX_NODES = 5_000  # 0 = no cap\n",
    "MAX_EDGES = 150_000 # 0 = no cap"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4",
   "metadata": {},
   "source": [
    "## 2. Scan Records CSV\n",
    "\n",
    "Stream through the CSV in chunks, extract genres/styles and release year\n",
    "from each row, and accumulate co-occurrence counts with year-based timeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5",
   "metadata": {},
   "outputs": [],
   "source": "import datetime, re\nfrom collections import Counter, defaultdict\nfrom itertools import combinations\nfrom typing import Dict, List, Optional, Tuple\n\nimport pandas as pd\nfrom tqdm import tqdm\n\nfrom traverse.processing.normalize import split_tags, pretty_label\n\n# ── Year parsing ─────────────────────────────────────────────────────\nYR4 = re.compile(r\"(?:^|[^0-9])(\\d{4})(?:[^0-9]|$)\")\n\ndef parse_year(v: object) -> Optional[int]:\n    if v is None: return None\n    s = str(v).strip()\n    try:\n        y = int(float(s))\n        if 0 < y < 10000: return y\n    except Exception: pass\n    m = YR4.search(s)\n    return int(m.group(1)) if m and 0 < int(m.group(1)) < 10000 else None\n\ndef clamp_year(y: Optional[int]) -> Optional[int]:\n    if y is None or y < YEAR_MIN or y > YEAR_MAX: return None\n    return y\n\ndef year_to_ts(y: Optional[int]) -> Optional[int]:\n    \"\"\"\n    Jan 1 of the given year as Unix epoch **milliseconds** (UTC).\n    Returns None for years before 1970 or invalid input.\n    \"\"\"\n    if y is None or not isinstance(y, int):\n        return None\n    if y < 1970 or y > 9999:\n        return None\n    dt = datetime.datetime(y, 1, 1, tzinfo=datetime.timezone.utc)\n    return int(dt.timestamp() * 1000)\n\ndef detect_col(colmap: Dict[str, str], *candidates: str) -> Optional[str]:\n    for c in candidates:\n        if c in colmap: return colmap[c]\n    return None\n\n# ── Accumulate co-occurrences ─────────────────────────────────────────\ncounts: Counter[Tuple[str, str]] = Counter()\nedge_first_year: Dict[Tuple[str, str], int] = {}\npoint_first_year: Dict[str, int] = {}\nfirst_label: Dict[str, str] = {}\ntag_category_counts: Dict[str, Counter] = defaultdict(Counter)\ntotal_rows = 0\ntagged_rows = 0\n\nreader = pd.read_csv(RECORDS_CSV, chunksize=CHUNKSIZE, dtype=\"string\",\n                      keep_default_na=True, na_filter=True)\n\nfor chunk in tqdm(reader, desc=\"Reading records\", unit=\"chunk\"):\n    total_rows += len(chunk)\n    colmap = {c.lower(): c for c in chunk.columns}\n\n    gcol = detect_col(colmap, \"genres\", \"genre\")\n    scol = detect_col(colmap, \"styles\", \"style\")\n    ycol = detect_col(colmap, \"release_year\", \"year\", \"releaseyear\",\n                      \"released_year\", \"release year\", \"released\")\n\n    for gval, sval, yval in zip(\n        chunk[gcol] if gcol else [\"\"] * len(chunk),\n        chunk[scol] if scol else [\"\"] * len(chunk),\n        chunk[colmap[ycol]] if ycol else [None] * len(chunk),\n    ):\n        genre_tags = split_tags(gval)\n        style_tags = split_tags(sval)\n        tags = genre_tags + style_tags\n        if not tags:\n            continue\n        tagged_rows += 1\n        y = clamp_year(parse_year(yval))\n\n        for t in set(tags):\n            if t not in first_label:\n                first_label[t] = pretty_label(t)\n            if y is not None:\n                cur = point_first_year.get(t)\n                if cur is None or y < cur:\n                    point_first_year[t] = y\n\n        # Track genre vs style category per tag\n        for t in set(genre_tags):\n            tag_category_counts[t][\"genre\"] += 1\n        for t in set(style_tags):\n            tag_category_counts[t][\"style\"] += 1\n\n        for a, b in combinations(sorted(set(tags)), 2):\n            counts[(a, b)] += 1\n            if y is not None:\n                cur = edge_first_year.get((a, b))\n                if cur is None or y < cur:\n                    edge_first_year[(a, b)] = y\n\nprint(f\"Scanned {total_rows:,} rows, {tagged_rows:,} with tags, \"\n      f\"{len(counts):,} unique edge keys\")"
  },
  {
   "cell_type": "markdown",
   "id": "a6",
   "metadata": {},
   "source": [
    "## 3. Build the Graph\n",
    "\n",
    "Apply thresholds and caps, then assemble points and links with\n",
    "release-year timeline (`first_seen_ts`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7",
   "metadata": {},
   "outputs": [],
   "source": "# Filter by min co-occurrence\nedges = [(a, b, w) for (a, b), w in counts.items() if w >= MIN_COOCCURRENCE]\nedges.sort(key=lambda x: x[2], reverse=True)\n\n# Cap nodes by weighted degree\nstrength: Dict[str, int] = defaultdict(int)\nfor a, b, w in edges:\n    strength[a] += w\n    strength[b] += w\n\nif MAX_NODES > 0:\n    keep = {n for n, _ in sorted(strength.items(),\n            key=lambda kv: kv[1], reverse=True)[:MAX_NODES]}\n    edges = [(a, b, w) for a, b, w in edges if a in keep and b in keep]\n\nif MAX_EDGES > 0 and len(edges) > MAX_EDGES:\n    edges = edges[:MAX_EDGES]\n\n# Assemble points\nnode_ids = set()\nfor a, b, _ in edges:\n    node_ids.add(a)\n    node_ids.add(b)\n\npoints = []\nfor nid in sorted(node_ids):\n    p = {\"id\": nid, \"label\": first_label.get(nid, nid)}\n    fy = point_first_year.get(nid)\n    if fy is not None:\n        ts = year_to_ts(fy)\n        if ts is not None:\n            p[\"first_seen_ts\"] = ts\n    # Majority-vote category from tag_category_counts\n    cats = tag_category_counts.get(nid)\n    if cats:\n        p[\"category\"] = cats.most_common(1)[0][0]\n    points.append(p)\n\n# Assemble links\nlinks = []\nfor a, b, w in edges:\n    lk = {\"source\": a, \"target\": b, \"weight\": w}\n    fy = edge_first_year.get((a, b))\n    if fy is not None:\n        ts = year_to_ts(fy)\n        if ts is not None:\n            lk[\"first_seen_ts\"] = ts\n    links.append(lk)\n\ngraph = {\"points\": points, \"links\": links}\n\n# Stats\npts_with_ts = sum(1 for p in points if \"first_seen_ts\" in p)\nlks_with_ts = sum(1 for l in links if \"first_seen_ts\" in l)\npts_with_cat = sum(1 for p in points if \"category\" in p)\ncat_values = {p.get(\"category\") for p in points} - {None}\nprint(f\"Graph: {len(points)} nodes ({pts_with_ts} with timeline, {pts_with_cat} with category), \"\n      f\"{len(links)} edges ({lks_with_ts} with timeline)\")\nprint(f\"Categories: {cat_values}\")"
  },
  {
   "cell_type": "markdown",
   "id": "a8",
   "metadata": {},
   "source": [
    "## 4. Export JSON and Serve\n",
    "\n",
    "Write the graph to the frontend's `dist/` directory, then start the\n",
    "built-in static server. Open the printed URL in your browser."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9",
   "metadata": {},
   "outputs": [],
   "source": "import json\nfrom traverse.cosmograph.server import serve, _default_dist_dir\n\n# Build output with meta for cluster field\nhas_category = any(\"category\" in p for p in points)\noutput = {}\nif has_category:\n    output[\"meta\"] = {\"clusterField\": \"category\"}\noutput[\"points\"] = graph[\"points\"]\noutput[\"links\"] = graph[\"links\"]\n\nout_path = _default_dist_dir() / \"cosmo_genres_records_timeline.json\"\nout_path.parent.mkdir(parents=True, exist_ok=True)\nout_path.write_text(json.dumps(output, indent=2), encoding=\"utf-8\")\nprint(f\"Wrote {out_path} ({len(points)} nodes, {len(links)} edges)\")\nif has_category:\n    print(\"  (includes meta.clusterField = 'category')\")\nprint()\nprint(\"Starting server — open in browser:\")\nprint(\"  http://127.0.0.1:8080/?data=/cosmo_genres_records_timeline.json\")\nprint()\nprint(\"Press Ctrl+C (or interrupt the kernel) to stop.\")\n\nserve(port=8080)"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py311",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}